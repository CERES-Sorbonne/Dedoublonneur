{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "540a0689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "from itertools import chain\n",
    "\n",
    "from colour import Color\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances as pairwise_cos_dist\n",
    "\n",
    "\n",
    "class avance():\n",
    "    \n",
    "    def __init__(self, liste_txt : list[str], NB_PIVOTS : int = 50) -> None:\n",
    "        self.liste_txt = liste_txt\n",
    "        self.NB_PIVOTS = NB_PIVOTS\n",
    "        \n",
    "        tqdm.write(\"\\nDédoublonage avancé\")\n",
    "        \n",
    "        with tqdm(total=2, desc = \"Calcul des vecteurs BoW\") as pbar:\n",
    "            docspivots = sample(list(enumerate(liste_txt)), NB_PIVOTS)\n",
    "            textespivots = {e for _, e in docspivots}\n",
    "            #On crée les vecteurs pour les textes (coordonnée n = occurences du mot de position n dans la liste de vocabulaire)\n",
    "            vocab = set(' '.join(textespivots).lower().split())\n",
    "            vectorizer = TfidfVectorizer(stop_words = (\"english\"), vocabulary = vocab)\n",
    "            pbar.update()\n",
    "            tf = vectorizer.fit_transform(liste_txt)\n",
    "            pbar.update()\n",
    "            self.arrtf = tf.toarray()\n",
    "            self.pivots = docspivots\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    def process(self, taillegroupe : int = 1000, sensibilite : float = 0.00001) -> None:\n",
    "        index_doublons , liste_doublons= [], []\n",
    "        liste_txt = self.liste_txt\n",
    "        self.taillegroupe = taillegroupe\n",
    "        avg = np.mean([len(e) for e in liste_txt])\n",
    "        #On sépare les textes en groupes de même taille, en fonction de leur longueur\n",
    "        from operator import itemgetter\n",
    "        tupliste = [(txt, i) for i, txt in enumerate(liste_txt)]\n",
    "        tupliste = sorted(tupliste, key=lambda tupliste: len(tupliste))\n",
    "\n",
    "        print(tupliste)\n",
    "        nbgroupes = 1 + (len(liste_txt) // taillegroupe)\n",
    "        groupes = [liste_txt[i*taillegroupe:(i+1)*taillegroupe] for i in range(nbgroupes)]\n",
    "\n",
    "        colors = [Color(\"blue\")] +list(Color(\"blue\").range_to(Color(\"green\"),nbgroupes))\n",
    "        \n",
    "        manualpbar = tqdm(groupes, desc=\"Calcul des vecteurs seconds / supression\")\n",
    "        for i, groupe in enumerate(manualpbar):\n",
    "            manualpbar.colour = colors[i].hex_l\n",
    "            #Array de tous les vecteurs 2, un texte par ligne comparé a chaque pivot par colonne\n",
    "            vect1_pivot = np.array([self.arrtf [i] * (avg / len(e)) for i, e in self.pivots], dtype = np.half)\n",
    "            vect1_non_pivot = np.array([self.arrtf [i] * (avg / len(e)) for i, e in enumerate(groupe)], dtype = np.half)\n",
    "            array_vecteurs = pairwise_cos_dist(vect1_non_pivot, vect1_pivot)\n",
    "            del vect1_non_pivot, vect1_pivot\n",
    "\n",
    "            #Matrice de la distance cosinus entre chaque texte du corpus, chaque intersection [x,y] donnant\n",
    "            #la distance entre les textes de rang x et y\n",
    "            matrice_cosine = pairwise_cos_dist(array_vecteurs)\n",
    "            del array_vecteurs\n",
    "\n",
    "            #Permet de faire abstraction de la distance nulle du texte n avec lui-même et ne pas le remonter comme doublon\n",
    "            for i in range(np.shape(matrice_cosine)[0]): \n",
    "                matrice_cosine[i,i] = 1\n",
    "            #Liste de toutes les distances faibles --> Doublons\n",
    "            lst_doublons = np.transpose(np.nonzero(matrice_cosine < sensibilite)).tolist()\n",
    "\n",
    "            del matrice_cosine\n",
    "\n",
    "            for e in lst_doublons:\n",
    "                lst_doublons.remove([e[1], e[0]])\n",
    "\n",
    "            #Nous permet de supprimer directement les doublons par l'index vu que l'élement reste a sa place (seuls ceux qui suivent ont étés supprimés)\n",
    "            doublons = sorted([e[1] for e in lst_doublons], reverse = True) \n",
    "            del lst_doublons\n",
    "\n",
    "            for db in doublons:\n",
    "                index_doublons.append(indices[(taillegroupe * i) + db])\n",
    "                liste_doublons.append(groupe.pop(db))\n",
    "\n",
    "            del doublons\n",
    "\n",
    "        self.liste_propre = list(chain.from_iterable(groupes))\n",
    "        self.index_doublons = index_doublons\n",
    "        self.liste_doublons = liste_doublons\n",
    "        tqdm.write(f\"Dédoublonage avancé : Il reste désormais {len(liste_txt)} articles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09e43bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "from itertools import chain\n",
    "\n",
    "from colour import Color\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances as pairwise_cos_dist\n",
    "\n",
    "\n",
    "class avance():\n",
    "    \n",
    "    def __init__(self, liste_txt : list[str], taillegroupe : int = 1000, NB_PIVOTS : int = 50) -> None:\n",
    "        self.liste_txt = liste_txt\n",
    "        self.taillegroupe = taillegroupe\n",
    "        self.NB_PIVOTS = NB_PIVOTS\n",
    "        \n",
    "        tqdm.write(\"\\nDédoublonage avancé\")\n",
    "        \n",
    "        with tqdm(total=2, desc = \"Calcul des vecteurs BoW\") as pbar:\n",
    "            avg = np.mean([len(e) for e in liste_txt])\n",
    "            docspivots = sample(liste_txt, NB_PIVOTS)\n",
    "\n",
    "            #On crée les vecteurs pour les textes (coordonnée n = occurences du mot de position n dans la liste de vocabulaire)\n",
    "            vocab = set(' '.join(docspivots).lower().split())\n",
    "            vectorizer = TfidfVectorizer(stop_words = (\"english\"), vocabulary = vocab)\n",
    "            pbar.update()\n",
    "            tf = vectorizer.fit_transform(liste_txt)\n",
    "            pbar.update()\n",
    "            arrtf = tf.toarray()\n",
    "\n",
    "            #On sépare les textes en groupes de même taille, en fonction de leur longueur\n",
    "            liste_txt = sorted(liste_txt, key=lambda liste_txt: len(liste_txt))\n",
    "            nbgroupes = 1 + (len(liste_txt) // taillegroupe)\n",
    "            groupes = [liste_txt[i*taillegroupe:(i+1)*taillegroupe] for i in range(nbgroupes)]\n",
    "\n",
    "        colors = list(Color(\"blue\").range_to(Color(\"green\"),nbgroupes))\n",
    "        tupliste = [(txt, i) for i, txt in enumerate(liste_txt)]\n",
    "        tupliste = sorted(tupliste, key=lambda tupliste: len(tupliste))\n",
    "        manualpbar = tqdm(groupes, desc=\"Calcul des vecteurs seconds / supression\")\n",
    "        doublons_tot = set()\n",
    "        cquoilesdoublons = set()\n",
    "        for i, groupe in enumerate(manualpbar):\n",
    "            manualpbar.colour = colors[i].hex_l\n",
    "            #Array de tous les vecteurs 2, un texte par ligne comparé a chaque pivot par colonne\n",
    "            vect1_pivot = np.array([arrtf[i] * (avg / len(e)) for i, e in enumerate(docspivots)], dtype = np.half)\n",
    "            vect1_non_pivot = np.array([arrtf[i] * (avg / len(e)) for i, e in enumerate(groupe)], dtype = np.half)\n",
    "            array_vecteurs = pairwise_cos_dist(vect1_non_pivot, vect1_pivot)\n",
    "            del vect1_non_pivot, vect1_pivot\n",
    "\n",
    "            #Matrice de la distance cosinus entre chaque texte du corpus, chaque intersection [x,y] donnant\n",
    "            #la distance entre les textes de rang x et y\n",
    "            matrice_cosine = pairwise_cos_dist(array_vecteurs)\n",
    "            del array_vecteurs\n",
    "\n",
    "            #Permet de faire abstraction de la distance nulle du texte n avec lui-même et ne pas le remonter comme doublon\n",
    "            for i in range(np.shape(matrice_cosine)[0]): \n",
    "                matrice_cosine[i,i] = 1\n",
    "            #Liste de toutes les distances faibles --> Doublons\n",
    "            lst_doublons = np.transpose(np.nonzero(matrice_cosine < 0.00001)).tolist()\n",
    "\n",
    "            del matrice_cosine\n",
    "\n",
    "            for e in lst_doublons:\n",
    "                lst_doublons.remove([e[1], e[0]])\n",
    "\n",
    "            #Nous permet de supprimer directement les doublons par l'index vu que l'élement reste a sa place (seuls ceux qui suivent ont étés supprimés)\n",
    "            doublons = sorted([e[1] for e in lst_doublons], reverse = True) \n",
    "            del lst_doublons\n",
    "\n",
    "            for db in doublons:\n",
    "                doublons_tot.add((taillegroupe * i) + db)\n",
    "                cquoilesdoublons.add(groupe.pop(db))\n",
    "\n",
    "            del doublons\n",
    "\n",
    "        self.liste_propre = list(chain.from_iterable(groupes))\n",
    "        self.index_doublons = doublons_tot\n",
    "        self.doublons = cquoilesdoublons\n",
    "        tqdm.write(f\"Dédoublonage avancé : Il reste désormais {len(liste_txt)} articles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7652709b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dédoublonage avancé\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85cf63d4abd14e5a84a261ad24268e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calcul des vecteurs BoW:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4decdbc4064a1599989d163552193a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calcul des vecteurs seconds / supression:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dédoublonage avancé : Il reste désormais 2871 articles.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def jsonread(chemin_fichier: str, y = \"utf-8-sig\"):\n",
    "    \"\"\"Charge un json et retourne l'objet appelé, prend en entrée le nom du fichier sans l'extension\"\"\"\n",
    "    with open(f\"{chemin_fichier}.json\", mode = \"r\", encoding = y) as j:\n",
    "            return json.load(j)\n",
    "        \n",
    "x = jsonread(\"a - 2022-07-04\")\n",
    "\n",
    "x = [e[\"texte\"] for e in x]\n",
    "\n",
    "y = avance(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8f7c5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{870001,\n",
       " 870011,\n",
       " 870012,\n",
       " 870048,\n",
       " 870063,\n",
       " 870068,\n",
       " 870074,\n",
       " 870081,\n",
       " 870117,\n",
       " 870142,\n",
       " 870174,\n",
       " 870225,\n",
       " 870232,\n",
       " 870313,\n",
       " 870314,\n",
       " 870327,\n",
       " 870383,\n",
       " 870427,\n",
       " 870466,\n",
       " 870471,\n",
       " 870532,\n",
       " 870680,\n",
       " 870772,\n",
       " 870775,\n",
       " 870826,\n",
       " 999001,\n",
       " 999011,\n",
       " 999012,\n",
       " 999048,\n",
       " 999063,\n",
       " 999068,\n",
       " 999074,\n",
       " 999081,\n",
       " 999117,\n",
       " 999142,\n",
       " 999174,\n",
       " 999225,\n",
       " 999232,\n",
       " 999313,\n",
       " 999314,\n",
       " 999327,\n",
       " 999383,\n",
       " 999427,\n",
       " 999466,\n",
       " 999471,\n",
       " 999532,\n",
       " 999680,\n",
       " 999772,\n",
       " 999775,\n",
       " 999826,\n",
       " 999949,\n",
       " 999993}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.index_doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b71a8f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee785bc4c7f4ec99b8f00c50a4da628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calcul des vecteurs seconds / supression:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'indices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mavance.process\u001b[1;34m(self, taillegroupe, sensibilite)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m lst_doublons\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m db \u001b[38;5;129;01min\u001b[39;00m doublons:\n\u001b[1;32m---> 80\u001b[0m     index_doublons\u001b[38;5;241m.\u001b[39mappend(\u001b[43mindices\u001b[49m[(taillegroupe \u001b[38;5;241m*\u001b[39m i) \u001b[38;5;241m+\u001b[39m db])\n\u001b[0;32m     81\u001b[0m     liste_doublons\u001b[38;5;241m.\u001b[39mappend(groupe\u001b[38;5;241m.\u001b[39mpop(db))\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m doublons\n",
      "\u001b[1;31mNameError\u001b[0m: name 'indices' is not defined"
     ]
    }
   ],
   "source": [
    "y = y.process()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
